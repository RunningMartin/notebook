# 并发编程-进程



现代操作系统执行一个程序需要在用户空间为进程分配一块独立的空间，操作系统负责调度与管理进程，操作系统调度进程执行时，以进程为单位分配系统资源。进程可以通过`fork`和`spawn`创建新的进程，但是新的进程有自己独立空间，因此需要进程通信机制(IPC)实现数据共享，如管道、信号、套接字、共享空间。

Python标准库`multiprocessing`提供操作进程的接口，这篇文章为大家介绍多进程操作方式。

## 进程创建

`multiprocessing`支持三种创建进程的方式，即：

- `spawn`：`spawn`创建的子进程只会从父进程中继承运行进程的必要资源，因此启动进程速度慢。Unix和Windows都支持`spawn`，Windows上默认采用`spawn`。
- `fork`：`fork`通过直接复制父进程来创建子进程，因此效率很高(有坑，多线程并不安全)，只有Unix支持`fork`，而且是Unix默认采用的方法。
- `forkserver`：`forkserver`会先创建一个单线程服务器进程，当需要创建新进程时，父进程请求服务器，从服务器进程中`fork`一个子进程，因此`fork`时是安全的，但不是所有的Unix平台都支持。

`multiprocessing`提供可以通过`set_start_method`或`get_context`方法设置创建进程的方式：

- `set_start_method(方法名)`：全局设置，作用于整个`multiprocessing`模块，因此不能被多次调用。
- `get_context(方法名)`：返回一个上下文，只作用于该上下文。

这里需要注意的是，从不同类型上下文中获取的对象不要混用， 如`fork` 上下文创建的锁不能传递给通过`spawn` 或 `forkserver`上下文启动的进程。

```python
def hello(name):
    from os import getpid
    import random, time
    time.sleep(random.randint(1, 5))
    print(f'hello {name}, I am {getpid()}')


from multiprocessing import get_context

if __name__ == '__main__':
    context = get_context('spawn')
    # 或直接multiprocessing.Process 通过默认方式创建进程
    process1 = context.Process(target=hello, args=('martin',))
    process2 = context.Process(target=hello, args=('martin',))
    for p in [process1, process2]:
        p.start()
    for p in [process1, process2]:
        p.join()
```

采用进程池启动多个类似进程

```python
#!/usr/bin/python
# -*- coding: UTF-8 -*-
import time
from multiprocessing import Pool


def hello(name):
    from os import getpid
    import time
    time.sleep(2)
    return f'hello {name}, I am {getpid()}'


def test_map(pool, names):
    start = time.perf_counter()
    # 返回的结果是和iterable的顺序相同
    # map是同步方法，直到其结果返回才会进行下一行代码
    print(pool.map(func=hello, iterable=names))
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_map_async(pool, names):
    """异步执行map"""
    start = time.perf_counter()
    # map_async命令下发后，会立即返回，不会在该处阻塞
    result = pool.map_async(func=hello, iterable=names)
    print("print map_async result")
    # 获取结果时会阻塞，查看源码map_async和map的区别在于，map中调用了get方法
    print(result.get())
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_imap_unordered(pool, names):
    """map_async的乱序版本"""
    start = time.perf_counter()
    # 乱序执行
    result = pool.imap_unordered(func=hello, iterable=names)
    for r in result:
        print(r)
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_apply(pool, names):
    """同步执行apply，apply函数只使用一个核，因此其耗时因此是单个耗时*len(names)"""
    start = time.perf_counter()
    result = [pool.apply(func=hello, args=(name,)) for name in names]
    print(result)
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_apply_async(pool, names):
    """异步执行apply方法"""
    start = time.perf_counter()
    # 异步执行
    result = [pool.apply_async(func=hello, args=(name,)) for name in names]
    for r in result:
        print(r.get())
    end = time.perf_counter()
    print(f"Used :{end-start}")


if __name__ == '__main__':
    with Pool(processes=4) as pool:
        names = ['martin', 'ana', 'liming', 'Kev']
        test_map(pool, names)
        test_map_async(pool, names)
        test_imap_unordered(pool, names)
        test_apply(pool, names)
        test_apply_async(pool, names)
```



计时方法：time.process_time、time.perf_counter

## 进程间通信

### 队列

```python
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

### 管道

当两端同时对管道的一端写入或读取时，数据可能损坏

```python
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```

### 共享内存

共享内存空间不到万不得已不要使用。`multiprocessing.sharedctypes`中记录了六种，其支持的类型由标准库`array.typecodes`或`ctypes`提供

- 数组
  - Array：返回一个数组，但是其有包含了锁，可以用于多进程同步，v.get_lock(),get_obj()将返回值。
  - RawArray：返回一个ctypes的数组。
- 值
  - Value
  - RawValue
- copy：从一个ctypes对象复制一个ctypes对象
- synchronized：将为一个ctypes对象添加一个锁，默认为RLock。

通过服务器进程进行管理

```python
# 调用Manager()创建一个manager对象控制一个服务器进程，能在本地和远程进行内存共享
# 支持list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Queue, Value和Array，其基类还支持注册新类型
```

## 进程间同步

`multiprocessing`提供锁来实现进程间同步，确保同一时刻只有一个进程执行该操作

- 互斥锁(Lock)：同一时刻只能有一个进程获取该锁。
- 递归锁(RLock)：同一个进程可以在不释放锁的情况下，多次获取该锁。

```python
from multiprocessing import Lock, RLock

def test_lock():
    lock = Lock()
    print("获取互斥锁")
    lock.acquire()
    print("再次获取互斥锁")
    lock.acquire()
    print("释放互斥锁")
    lock.release()

def test_rlock():
    rlock = RLock()
    print("获取递归锁")
    rlock.acquire()
    print("再次获取递归锁")
    rlock.acquire()
    print("释放递归锁")
    rlock.release()

if __name__ == '__main__':
    test_rlock()
    test_lock()
```

日志：`multiprocessing.get_logger`

多进程编程指南

- 使用管道或队列替代共享内存。
- 确保所有参数可序列化。
- 不要在多线程中使用代理对象，如果使用必须通过锁来提供保护。
- 手动join 僵尸进程(僵尸进程：已经完成但是没有join的进程，当启动新进程、调用`active_children()`、is_alive都会join进程，但是最好还是手动join)
- 继承而不是序列化与反序列化？？？？？
- 避免调用进程的`terminate()`方法：因为这个方法会导致多进程之间的共享资源阻塞或不可用。
- 使用管道时，put元素的进程只有在将数据从管道中取出后，才能被join，不然会阻塞(非守护进程会被自动join)

```
from multiprocessing import Process, Queue

def f(q):
    q.put('X' * 1000000)

if __name__ == '__main__':
    queue = Queue()
    p = Process(target=f, args=(queue,))
    p.start()
    p.join()                    # this deadlocks
    obj = queue.get()
```

- 显式传递资源给子进程(通过fork的方式，子进程能获取父进程的全局资源，但是如果子进程不能确保资源的活跃性，父进程的垃圾回收机制可能会将资源回收)。
- 使用类文件对象替代`sys.stdin`时，需要注意当多个进程调用该类对象的close方法时，可能会导致相同的数据被多次写书对象，可以通过保存自己的pid，通过判断当前的pid，决定是否丢弃缓存，从而确保fork安全。

## 书籍

- 多线程中fork的坑[谨慎使用多线程中的fork](https://www.cnblogs.com/liyuan989/p/4279210.html)

- 《linux多线程服务端编程》
