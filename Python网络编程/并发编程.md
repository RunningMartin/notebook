# 并发编程

## 前言

现代操作系统中每个程序都是运行在某个进程的上下文中。上下文由程序正确运行所需的资源组成，包含存储器中的程序的代码和数据，它的栈、通用目的寄存器的内容、程序计数器（PC）、环境变量以及打开的文件描述符的集合。操作系统以进程为单位分配资源，初始化时，会在内存中为进程分配一块独立的空间，进程由操作系统负责调度与管理。

进程主要提供给上层应用两个抽象：

- 独立的逻辑控制流：它提供一个抽象，即该进程独占处理器。
- 私有的虚拟地址空间：它提供一个抽象，即独占存储系统。

线程是运行在进程上下文中的逻辑流，由内核自动调度。每个线程都有自己的线程上下文，包含一个唯一的整数线程ID、栈、栈指针、程序计数器（PC）、通用目的寄存器和条件码。每个线程和运行在同一进程中的其他线程共享进程上下文。

综上所述，我们可以总结出进程是资源管理的最小单位，线程是程序只需的最小单位。

Linux中通过`fork`创建的子进程，将获得父进程的虚拟地址空间的拷贝，包括文本、数据和`bss`段、堆以及用户栈等，因此子进程可以读写父进程打开的文件，父子进程最大的区别是`PID`(进程ID)不同。

使用并发时，切忌不要将多线程和多进程结合使用：

- 如果父进程采用多线程，子进程只会复制当前线程。
- 大多数操作系统中，基于性能考虑，锁基本上都是在用户态实现(内核态实现会多一个系统调用)，因此`fork`时会将父进程拥有的锁复制到子进程。如果`X`线程对锁`A`执行加锁操作后，`Y`线程执行了`fork`操作，则子进程中只会复制`Y`线程。子进程中`A`锁的将不再会被释放，如果子进程中任意线程对`A`锁执行加锁操作，将发生死锁。而且即使能保证子进程逻辑代码中不会发生死锁，也不能保证所使用到的库函数不会出现这种情况，C语言中常见的`malloc`、`printf`都是通过互斥锁实现的。

概念：

- 并发：CPU在同一时间段内运行多个进程，但在同一时刻中只能运行一个进程。
- 并行：多个CPU同时处理多个任务。

## 进程

### 进程创建

`multiprocessing`支持三种创建进程的方式，即：

- `spawn`：通过`spawn`创建的子进程从父进程中继承运行进程的必要资源，因此启动进程速度慢。Unix和Windows都支持`spawn`，Windows上默认采用`spawn`。
- `fork`：`fork`通过直接复制父进程来创建子进程，因此效率很高(有坑，多线程并不安全)，只有Unix支持`fork`，而且是Unix默认采用的方法。
- `forkserver`：`forkserver`会先创建一个单线程服务器进程，当需要创建新进程时，父进程请求服务器，从服务器进程中`fork`一个子进程，因此`fork`时是安全的，但不是所有的Unix平台都支持。

`multiprocessing`提供两个API，用于设置创建进程的方式：

- `set_start_method(方法名)`：全局设置，作用于整个`multiprocessing`模块，因此不能被多次调用。
- `get_context(方法名)`：返回一个上下文，只作用于该上下文。

这里需要注意的是，从不同类型上下文中获取的对象不要混用， 如`fork` 上下文创建的锁不能传递给通过`spawn` 或 `forkserver`上下文启动的进程。

```python
def hello(name):
    from os import getpid
    import random, time
    time.sleep(random.randint(1, 5))
    print(f'hello {name}, I am {getpid()}')


from multiprocessing import get_context

if __name__ == '__main__':
    context = get_context('spawn')
    # 或直接multiprocessing.Process 通过默认方式创建进程
    process1 = context.Process(target=hello, args=('martin',))
    process2 = context.Process(target=hello, args=('martin',))
    for p in [process1, process2]:
        p.start()
    for p in [process1, process2]:
        p.join()
```

`Python`还提供了进程池用于限制进程创建数量：

```python
import time
from multiprocessing import Pool

def hello(name):
    from os import getpid
    import time
    time.sleep(2)
    return f'hello {name}, I am {getpid()}'


def test_map(pool, names):
    start = time.perf_counter()
    # 返回的结果是和iterable的顺序相同
    # map是同步方法，直到其结果返回才会进行下一行代码
    print("print map result")
    print(pool.map(func=hello, iterable=names))
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_map_async(pool, names):
    """异步执行map"""
    start = time.perf_counter()
    # map_async命令下发后，会立即返回，不会在该处阻塞
    result = pool.map_async(func=hello, iterable=names)
    print("print map_async result")
    # 获取结果时会阻塞，查看源码map_async和map的区别在于，map中调用了get方法
    print(result.get())
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_imap_unordered(pool, names):
    """map_async的乱序版本"""
    start = time.perf_counter()
    # 乱序执行
    result = pool.imap_unordered(func=hello, iterable=names)
    for r in result:
        print(r)
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_apply(pool, names):
    """同步执行apply，apply函数只使用一个核，因此其耗时因此是单个耗时*len(names)"""
    start = time.perf_counter()
    print("print apply")
    result = [pool.apply(func=hello, args=(name,)) for name in names]
    print(result)
    end = time.perf_counter()
    print(f"Used :{end-start}")


def test_apply_async(pool, names):
    """异步执行apply方法"""
    start = time.perf_counter()
    print("print apply async")
    # 异步执行
    result = [pool.apply_async(func=hello, args=(name,)) for name in names]
    for r in result:
        print(r.get())
    end = time.perf_counter()
    print(f"Used :{end-start}")


if __name__ == '__main__':
    with Pool(processes=4) as pool:
        names = ['martin', 'ana', 'liming', 'Kev']
        test_map(pool, names)
        test_map_async(pool, names)
        test_imap_unordered(pool, names)
        test_apply(pool, names)
        test_apply_async(pool, names)
```

### 进程通信

进程可以通过`fork`和`spawn`创建新的进程，但是新进程有自己独立空间，因此进程之间通信只能采用进程通信机制(IPC)，如管道、信号、套接字、共享空间。Python的标准库`multiprocessing`支持队列和管道。

#### 队列

```python
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

#### 管道

当两端同时对管道的一端写入或读取时，数据可能损坏。

```python
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```

### 进程同步

`multiprocessing`提供锁来实现进程间同步，确保同一时刻只有一个进程执行该操作：

- 互斥锁(Lock)：同一时刻只能有一个进程获取该锁。
- 递归锁(RLock)：同一个进程可以在不释放锁的情况下，多次获取该锁。

```python
from multiprocessing import Lock, RLock

def test_lock():
    lock = Lock()
    print("获取互斥锁")
    lock.acquire()
    print("再次获取互斥锁")
    lock.acquire()
    print("释放互斥锁")
    lock.release()

def test_rlock():
    rlock = RLock()
    print("获取递归锁")
    rlock.acquire()
    print("再次获取递归锁")
    rlock.acquire()
    print("释放递归锁")
    rlock.release()
    rlock.release()

if __name__ == '__main__':
    test_rlock()
    test_lock()
```

## 线程

### Thread

创建一个线程有两种方法：

- 使用任务初始化一个`Thread`对象：`Thread(target=func,args=元组形式参数信息)`
- 继承`Thread`，重写`__init__`和`run`方法，`run`方法中写入线程的任务。

初始化线程时，有个`dameon`参数用于设置守护线程，通常情况下，主线程创建的子线程如果没有结束，则主线程也不能结束，但如果主线程为守护线程，则可以直接退出。

常用方法：

- start()：启动线程。
- run()：线程的任务
- join([timeout])：阻塞当前线程，直到目标线程结束，若指定了timeout，则会在这段时间内判断线程是否存活，超时，返回None，因此要结合is_alive()共同判断。
- is_alive()：判断线程是否存活。
- ident：线程ID，如果还未启动，则为None；启动状态下返回非零整数。

### 线程同步

#### 锁

多线程协同时，如果同时访问一个资源，可能会带来难以预料的问题，因此`threading`提供锁来保证任意时刻访问该资源的线程只有一个，锁有两类：

- 互斥锁`Lock`：锁没有释放前，拥有锁的线程不能多次获取。
- 递归锁`RLock`：锁没有释放前，拥有锁的线程能多次获取，内部有计数器统计被获取了多少次，因此获取与释放次数必须相同，直到为0，才能被其他线程获取。
- 使用锁时，要防止多个函数使用同一个锁又互相调用引发死锁。

锁对象的常用方法：

- acquire(blocking=True,timeout=-1)：获取锁，如果blocking为True，则会阻塞，直到获取该锁，timeout会现在阻塞时长。获取到锁，返回True，没有获取，返回False。
- release()：释放锁。

#### 条件对象

条件对象`Condition`其实是一个高级锁，其支持在初始化时，传入一个锁，如果没有传入，默认创建一个RLock锁，因此可以通过`acquire`和`release`来获取、释放锁。支持使用上下文管理器。

上下文管理协议：

```
    def __enter__(self):
        #进入上下文时的处理

    def __exit__(self, exc_type, exc_val, exc_tb):
        #退出时的处理，返回True则不会抛异常
```

- acquire()：获取锁
- release()：释放锁
- wait([timeout])：释放自己拥有的锁后主动挂起，等待被唤醒或超时。当唤醒后，必须重新获得锁后，才能继续执行。
- wait_for(条件函数名,[timeout])：条件不成立，则一直等待。
- notify()：唤醒一个被该条件阻塞的线程。
- notify_all()：唤醒所有被该条件阻塞的线程。

```python
import threading
from queue import Queue
import time


class Producer(threading.Thread):
    def __init__(self, name, condition, queue, start, end):
        super().__init__()
        self.name = name
        self.condition = condition
        self.queue = queue
        self.start_index = start
        self.end_index = end

    def run(self):
        for i in range(self.start_index, self.end_index):
            if self.condition.acquire():
                if self.queue.full():
                    print("{} wait".format(self.name))
                    self.condition.wait()
                else:
                    print('{} produced {}'.format(self.name, i))
                    self.queue.put(i)
                    self.condition.notify()
            self.condition.release()
            time.sleep(1)


class Consumer(threading.Thread):
    def __init__(self, name, condition, queue):
        super().__init__()
        self.name = name
        self.condition = condition
        self.queue = queue

    def run(self):
        i = 0
        while True:
            if self.condition.acquire():
                if self.queue.empty():
                    self.condition.wait()
                else:
                    i = self.queue.get()
                    print('{} consume {}'.format(self.name, i))
                    self.condition.notify()

            self.condition.release()
            time.sleep(1)


if __name__ == '__main__':
    queue = Queue(5)
    condition = threading.Condition()
    producer1 = Producer('producer1', condition, queue, 0, 20)
    producer2 = Producer('producer2', condition, queue, 20, 40)
    producer3 = Producer('producer3', condition, queue, 40, 60)
    producer4 = Producer('producer4', condition, queue, 60, 80)
    producer5 = Producer('producer5', condition, queue, 80, 100)

    consumer = Consumer('consumer', condition, queue)
    for t in [producer1, producer2, producer3, producer4, producer5, consumer]:
        t.start()

    for t in [producer1, producer2, producer3, producer4, producer5, consumer]:
        t.join()

```

#### 信号量

信号量( `Semaphore`)用于控制公共资源的访问，初始化时可以指定同时访问资源的线程数，默认值为1，内部通过维护一个计数器，当计数器值为0时，申请该资源的线程将阻塞。

- `acquire`：信号量计数器-1。
- `release`：计数器+1。

`BoundedSemaphore`是`Semaphore`的特殊版本，如果计数器的值超过了初始值，则会抛出异常`ValueError`。

#### 事件

事件是一种基于`Condition`的简单通信机制:

- 调用`set()`将其设置为`True`，并唤醒所有等待该事件的线程。
- `clear()`将其设置为`False`。
- `wait()`如果事件是`True`则立即返回，否则会阻塞，直到为`True`。

### 线程通信

多线程处理资源共享时，虽然可以使用同步机制解决线程并发问题，但最佳方案还是使用队列。`queue`模块提供了三种基于线程锁的队列，支持线程之间信息交互。

- FIFO队列(先进先出)：`Queue`
- LIFO队列(后进先出)：`LifoQueue`
- 优先级队列：`PriorityQueue`

支持如下方法：

- get(block=True,timeout=None)：获取一个元素，如果不能获取，默认会阻塞，如果block为false或触发异常，当获取不到元素时，触发Empty异常
- put(item,block=True,timeout=None)：往队列中存入数据，如果block为fasle不能插入数据或超时，则触发Full异常。
- task_one()：内部维护了一个`Condition`和任务计数器，表明当前任务已经完成，队列可以被其他任务处理了。
- join()：如果队列中还有任务未完成，则阻塞。

```python
# -*- coding:utf-8 -*-
import threading
import queue
from time import sleep


class ConsumerThread(threading.Thread):
    def __init__(self, que):
        super().__init__()
        self.queue = que

    def run(self):
        while True:
            sleep(0.1)
            item = self.queue.get()
            print(item)
            self.queue.task_done()


class ProducerThread(threading.Thread):
    def __init__(self, que):
        super().__init__()
        self.queue = que

    def run(self):
        for x in range(10):
            self.queue.put(x)


if __name__ == '__main__':
    que = queue.Queue()

    tasks = [ConsumerThread(que) for x in range(2)]

    for t in tasks:
        t.setDaemon(True)
        t.start()

    producer = ProducerThread(que)
    producer.start()
    que.join()
    print('all done')
```

### thread local

多线程中，如果使用全局变量，就意味着必须为该变量加锁，因此最佳方法是采用局部变量，但是局部变量如果通过层层传递，就会写的非常丑陋，最佳的方法是通过全局变量dict，以线程自身作为key，获取相关的对象。

threadlocal提供了该功能。

```python
import threading

# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
```

## concurrent.futures

`concurrent.futures`模块提供异步执行回调相关的API，主要有线程池和进程池：

- `ThreadPoolExecutor`
- `ProcessPoolExecutor`

```python
import concurrent.futures
import math
import time

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]


def is_prime(n):
    if n < 2:
        return False
    if n == 2:
        return True
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True


if __name__ == '__main__':
    start = time.perf_counter()
    with concurrent.futures.ProcessPoolExecutor(3) as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))
    print("multi process pool used:{}".format(time.perf_counter() - start))

    start = time.perf_counter()
    with concurrent.futures.ThreadPoolExecutor(3) as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))
    print("multi thread pool used:{}".format(time.perf_counter() - start))

    # 异步执行
    start = time.perf_counter()
    with concurrent.futures.ThreadPoolExecutor(3) as executor:
        results = {}
        for number in PRIMES:
            prime = executor.submit(is_prime, number)
            results[number] = prime

        for number, prime in results.items():
            print('%d is prime: %s' % (number, prime.result()))
    print("multi thread pool used:{}".format(time.perf_counter() - start))
```

## 协程

在解决并发问题的过程中，出现了进程、线程和协程。其中线程解决进程上下文切换带来的消耗，而协程是为了解决线程启动、管理和同步锁的开销。协程中以事件为单位，通过调度器来调度相应的事件。

协程执行有三种方法：

- `await`：程序阻塞在当前位置，进入被调用的协程函数，执行完毕返回后再继续。

  ```python
  import asyncio
  
  import time
  
  
  async def crawl_page(url):
      print('crawling {}'.format(url))
      sleep_time = int(url.split('_')[-1])  # 通过睡眠来模拟爬页面带来的开销
      await asyncio.sleep(sleep_time)
      print('OK {}'.format(url))
  
  
  async def main(urls):
      for url in urls:
          await crawl_page(url)
  
  
  start = time.perf_counter()
  asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))
  print('used {}'.format(time.perf_counter() - start))
  
  ```

- 通过`asyncio.create_task`创建事件。

  ```python
  import asyncio
  import time
  
  async def crawl_page(url):
      print('crawling {}'.format(url))
      sleep_time = int(url.split('_')[-1])  # 通过睡眠来模拟爬页面带来的开销
      await asyncio.sleep(sleep_time)
      print('OK {}'.format(url))
  
  
  async def main(urls):
      tasks = [asyncio.create_task(crawl_page(url)) for url in urls]
      for task in tasks:
          await task
  
  start = time.perf_counter()
  asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))
  print('used {}'.format(time.perf_counter() - start))
  ```

- 通过`asyncio.gather()`启动事件。

  ```python
  import asyncio
  import time
  
  async def crawl_page(url):
      print('crawling {}'.format(url))
      sleep_time=int(url.split('_')[-1])# 通过睡眠来模拟爬页面带来的开销
      await asyncio.sleep(sleep_time)
      print('OK {}'.format(url))
  
  async def main(urls):
      tasks=[asyncio.create_task(crawl_page(url)) for url in urls]
      await asyncio.gather(*tasks)
  
  start = time.perf_counter()
  asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))
  print('used {}'.format(time.perf_counter() - start))
  ```

### 协程运行流程

`await`有两个作用：

- 交出控制权。
- 等待被调用的协程函数执行完毕返回。

同步执行：因为没有创建事件，只有一个事件可供调度，因此是同步执行。

```python
import asyncio

async def work_1():
    print('work_1 start')
    await asyncio.sleep(1)
    print('work_1 end')

async def work_2():
    print('work_2 start')
    await asyncio.sleep(2)
    print('work_2 end')

async def main():
    print('before await')
    await work_1()
    print('awaited work_1')
    await work_2()
    print('awaited work_2')

start = time.perf_counter()
asyncio.run(main())
print('used {}'.format(time.perf_counter() - start))
```

异步执行：

```python
import asyncio


async def work_1():
    # 4、work_1获得控制权
    print('work_1 start')
    # 5、交出了控制权，等待asyncio.sleep运行完毕
    await asyncio.sleep(1)
    # 8、asyncio.sleep 完成，work_1获得控制权
    print('work_1 end')


async def work_2():
    # 6、work_2获得控制权
    print('work_2 start')
    # 7、交出了控制权，等待asyncio.sleep运行完毕，没有可执行的事件，事件调度器进入等待
    await asyncio.sleep(2)
    # 11、asyncio.sleep 完成，work_2获得控制权
    print('work_2 end')


async def main():
    # 2、创建任务，并加入事件循环中。
    task1=asyncio.create_task(work_1())
    task2=asyncio.create_task(work_2())
    print('before await')
    # 3、main交出控制权，事件调度器调度task1执行，等待task1完成
    await task1
    # 9、task1完成，main获得控制权
    print('awaited work_1')
    # 10、等待task2完成
    await task2
    # 12、task2完成，main获得控制权
    print('awaited work_2')

# 1、程序进入main函数，启动事件循环。
start = time.perf_counter()
asyncio.run(main())
print('used {}'.format(time.perf_counter() - start))
```

### 进阶

拦截协程异常:`asyncio.gather(*aws, loop=None, return_exceptions=True)`，返回的值中有异常信息。如果不用`return_exceptions`，则需要手动捕获异常。

```python
import asyncio
import random


async def consumer(queue, id):
    while True:
        val = await queue.get()
        print('consumer {} consume {}'.format(id, val))
        await asyncio.sleep(1)


async def producer(queue, id):
    for i in range(5):
        val = random.randint(1, 10)
        await queue.put(val)
        print('producer {} produce {}'.format(id, val))
        await asyncio.sleep(1)


async def main():
    queue = asyncio.Queue()
    consumer1 = asyncio.create_task(consumer(queue, 1))
    consumer2 = asyncio.create_task(consumer(queue, 2))
    producer3 = asyncio.create_task(producer(queue, 1))
    producer4 = asyncio.create_task(producer(queue, 2))

    await asyncio.sleep(15)
    consumer1.cancel()
    consumer2.cancel()
    await asyncio.gather(consumer1, consumer2, producer3, producer4, return_exceptions=True)

asyncio.run(main())
```

线程和协程的区别：协程是单线程的，由用户决定在哪里交出控制权，因此写协程时，必须在脑海中有清晰的事件循环概念，知道什么时候该暂停、什么时候等待I/O。

协程如何实现回调函数：通过task调用`add_done_callback`添加回调函数。

### loop

```python
import asyncio

import time

now = lambda: time.time()

async def do_some_work(x):
    print('Waiting: ', x)

    await asyncio.sleep(x)
    return 'Done after {}s'.format(x)

start = now()

coroutine1 = do_some_work(1)
coroutine2 = do_some_work(2)
coroutine3 = do_some_work(4)

tasks = [
    asyncio.ensure_future(coroutine1),
    asyncio.ensure_future(coroutine2),
    asyncio.ensure_future(coroutine3)
]

loop = asyncio.get_event_loop()
# wait接收一个列表，返回done,pending任务列表,可以通过task.result()获取
# gather接收一堆task 直接返回结果，
# wait和gather用于创建一个协程对象
loop.run_until_complete(asyncio.wait(tasks))

for task in tasks:
    print('Task ret: ', task.result())

print('TIME: ', now() - start)
```
